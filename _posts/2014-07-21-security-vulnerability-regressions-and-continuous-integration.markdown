---
layout: post
title: Security vulnerability regressions and continuous integration
date: '2014-07-21'
categories:
- Software
tags:
- cucumber
- ruby
- security
- spacewalk
- suse
- susemanager
---

When we released [SUSE Manager](https://www.suse.com/products/suse-manager/) 1.2 back in 2011, one of the first internal components [we open-sourced&nbsp;was our internal testsuite](http://www.redhat.com/archives/spacewalk-devel/2011-January/003213.html)&nbsp;and I have [written a bit about it before](http://duncan.mac-vicar.com/2012/05/04/suse-manager-a-year-later-retrospective-ii-backstage/). Our process was from the beginning completely automated to the extreme. If you do a git commit, you can expect the installable .iso file with the bootable SUSE Manager appliance to appear in a folder after some time. Every couple of hours, this .iso is auto-installed in a reference server and reference client servers, which we use to quickly checkout "things", and additionally, it is installed on a server, where a list of hundred of tests is performed using a human description of the features, and executed in a real Web Browser.

![SUSE Manager Continuous Integration]({{ site.baseurl }}/assets/cucumber-testing-diagram.png)

We could not do this without a bunch of great technologies and products: [git](http://git-scm.com) and [github](http://github.com), [Jenkins](http://jenkins-ci.org/), where we pull the git repositories and build tarballs + rpm spec files. The [Open Build Service](http://openbuildservice.org), where we submit all the tarballs + rpm .spec files and they get built together as a project and later thanks to the [KIWI](http://doc.opensuse.org/projects/kiwi/doc/) technology inside the Build Service, turned into an appliance. [KVM](http://www.linux-kvm.org/page/Main_Page) and [libvirt](http://libvirt.org/), which we use to run the latest version of our product, and then of course [Cucumber](http://cukes.info), which allows us to describe features in a high-level language, and then implement it to be run on [Firefox](http://www.mozilla.org) using [WebDriver](http://docs.seleniumhq.org/projects/webdriver/). But recently we have added another set of features to the testsuite: Security regressions. It was [Victor](https://github.com/vpereira), one of our security engineers, who approached us to talk about using [OWASP ZAP](https://www.owasp.org/index.php/OWASP_Zed_Attack_Proxy_Project) to find vulnerabilities and harden the product ( [his presentation](https://speakerdeck.com/vpereira/security-regression-tests-with-zap-proxy)). The project describes itself as:

> The Zed Attack Proxy (ZAP) is an easy-to-use, integrated penetration-testing tool. It locates vulnerabilities in web applications, and helps&nbsp;you build secure apps. Designed for use by people with a wide range of security experience, it’s also suited for developers and functional&nbsp;testers who are new to penetration testing. With its automated scanner&nbsp;and powerful REST API, ZAP fits seamlessly into your continuous&nbsp;integration environment, allowing you to automate the finding of&nbsp;common issues while you’re still in development.

As soon as he explained how it worked: the passive tests used a proxy, the pieces of the puzzle clicked: we could run the whole testsuite through the proxy, and add the results analysis as part of one of the last steps of the tests. Those tools produce lot of false positives, so we could keep a "baseline" or "ignore list" and still get the testsuite to fail when new vulnerabilities that we haven't analysed pop up. Integrating it was straightforward. We packaged the ZAP suite as an rpm that gets installed in the machine that is deployed to run the testsuite against the product. The testsuite code is mostly ruby, so we used the [owasp\_zap gem](https://github.com/SUSE/owasp_zap) for which Victor himself is the author. The testsuite starts the proxy, configures Firefox to go through it and run all the tests. There is an optional step before retrieving the results of performing an active attack on the host (eg. finding SQL injections). The last test retrieves the results, compares against the baseline and then embeds the report in the feature steps, which is considered failed or passed whether new vulnerabilities are found. [Martin](https://github.com/mseidl) found a way to organize the results better: we normalize some POST parameters that change all the time so that they don't result in separate incidents and we also group (using the URL path) the incidents per application component.

![Cucumber report]({{ site.baseurl }}/assets/89ab22c8-0cde-11e4-8c10-0e702eb9e7ba.png)

We have just started with this: The initials runs immediately showed the need to harden our apache and tomcat configurations but we still have lot of information to analyse and parameters to tune. For example, our developers have found problems&nbsp;in unreleased code commits which were pointed out and fixed immediately, but it would be nice if we could tune the scanner so that those real-life scenarios are detected by the tool, in case a similar mistake is repeated.

